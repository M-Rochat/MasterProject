{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28c1402-4d56-4627-ab69-98aba47a2e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import evaluate\n",
    "from transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainingArguments, pipeline\n",
    "from argparse import ArgumentParser\n",
    "import torch\n",
    "from torch import cuda\n",
    "import numpy as np\n",
    "from datasets import load_dataset, DownloadMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb40e7f-9093-4796-bf40-23eb267c3bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--seed\", default=42)\n",
    "parser.add_argument(\"--model_name\", default='facebook/bart-base')\n",
    "parser.add_argument(\"--dataset_dir\", default=\"modified_dataset/\")\n",
    "parser.add_argument(\"--reset_cache\", action='store_true')\n",
    "parser.add_argument(\"--device\", default='cuda' if cuda.is_available() else 'cpu')\n",
    "parser.add_argument(\"--output_dir\", default=\"./seq_to_seq\", help=\"The output directory\")\n",
    "parser.add_argument(\"--overwrite_output_dir\", default=True, help=\" overwrite the content of the output directory\")\n",
    "parser.add_argument(\"--num_train_epochs\", default=3)  # number of training epochs\n",
    "parser.add_argument(\"--per_device_train_batch_size\", default=32)  # batch size for training\n",
    "parser.add_argument(\"--per_device_eval_batch_size\", default=64)  # batch size for evaluation\n",
    "parser.add_argument(\"--eval_steps\", default=400)  # Number of update steps between two evaluations.\n",
    "parser.add_argument(\"--save_steps\", default=800)  # after # steps model is saved\n",
    "parser.add_argument(\"--warmup_steps\", default=500)  # number of warmup steps for learning rate scheduler\n",
    "parser.add_argument(\"--prediction_loss_only\", default=True)\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a8a35e-d58e-4702-acb0-5963e33a74f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir= \"modified_dataset/\"\n",
    "data_files = {\"train\": dataset_dir + \"train.tsv\", \"test\": dataset_dir + \"test.tsv\",\n",
    "              \"dev\": dataset_dir + \"dev.tsv\"}\n",
    "dataset = load_dataset('csv', data_files=data_files, delimiter='\\t',\n",
    "                       download_mode= DownloadMode.REUSE_DATASET_IF_EXISTS)\n",
    "dataset['train'] = dataset['train'].select(range(10000))\n",
    "print(\"Dataset loaded\", len(dataset['train']), dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f30c3ec-7c53-448d-b6ba-6a48de2649f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name)\n",
    "model.to(args.device)\n",
    "print(\"Model + tokenizer downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925a0280-e27b-4812-95bd-650bf6c05ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(['This is awesome, Very much','a'],truncation=True,max_length=4, return_overflowing_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b778350f-117c-415c-89f3-b5da31299692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"dill<0.3.5\" #fix to hash error but not sure if it really fixes the issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a70e8-0675-4bc7-85ed-06d48905d5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 64\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples['head'], text_target=examples['tail'], max_length=max_seq_length,\n",
    "                             truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,  # num_proc=num_proc,\n",
    "    remove_columns=['head', 'tail'],\n",
    "    load_from_cache_file=True\n",
    ")\n",
    "\n",
    "print('Tokenization done')\n",
    "print(tokenized_dataset['train'].shape)\n",
    "print(tokenized_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50461299-0dc3-466e-a397-b57a094dff0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "batch = data_collator([tokenized_dataset[\"train\"][i] for i in range(10)])\n",
    "print(tokenizer.pad_token)\n",
    "print(batch.keys())\n",
    "print(batch['input_ids'])\n",
    "print(batch['labels'])\n",
    "print(batch['decoder_input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8516101e-a032-4425-b5c7-ec9104a43781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d26c5f-1282-4d2b-9d40-4b0006533a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir= args.output_dir, \n",
    "    report_to = 'none',\n",
    "    evaluation_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset= tokenized_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba2c7fc-52d0-4680-b04b-d54de01911d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_result = trainer.train()\n",
    "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "metrics = train_result.metrics\n",
    "\n",
    "metrics[\"train_samples\"] = len(train_dataset)\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9105421-f3ee-4109-8dc3-a8e15694ff32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.evaluate(max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03464748-7176-4693-83c9-cf3fe92eecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "input_ids = tokenizer.encode(\"I give you an apple. I am\", return_tensors=\"pt\")\n",
    "print(input_ids)\n",
    "print(tokenizer.decode(input_ids[0]))\n",
    "generations = model.generate(input_ids=input_ids.to(args.device), max_new_tokens=50, do_sample=True,num_return_sequences=10)\n",
    "for gen in generations:\n",
    "    new_text = tokenizer.decode(gen)\n",
    "    print(new_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7671ee-0587-4e82-86cc-cc8723a85cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer= tokenizer)\n",
    "qa_pipeline(\"I give you an apple. I am\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ae698b-7e55-4a80-bf1c-858c576bb01c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
