{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0fede98-41c8-4073-9d06-b840ac2ece0a",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c27ca958-9576-45b0-9998-ea983e622807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import cuda\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import evaluate\n",
    "from transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainingArguments, pipeline, AdamW, get_scheduler\n",
    "from argparse import ArgumentParser\n",
    "import numpy as np\n",
    "from datasets import load_dataset, DownloadMode\n",
    "from tqdm.auto import tqdm\n",
    "from script.rec_adam import RecAdam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035d4171-a787-4048-be3a-4b224cc5114f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## model + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12a9cb63-f852-4990-b0c1-52d865b62193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TADA!\n",
      "TADA!\n",
      "TADA!\n",
      "TADA!\n",
      "TADA!\n",
      "TADA!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['decoder.layers.4.adapters.4.sequential.2.bias', 'decoder.layers.1.adapters.3.sequential.0.bias', 'decoder.layers.4.adapters.1.sequential.2.weight', 'decoder.layers.4.ln_3.weight', 'decoder.layers.4.adapters.1.sequential.0.weight', 'decoder.layers.0.ln_3.bias', 'decoder.layers.2.adapters.3.sequential.2.bias', 'decoder.layers.5.adapters.3.sequential.2.bias', 'decoder.layers.5.adapters.4.sequential.0.weight', 'decoder.layers.2.adapters.4.sequential.2.bias', 'decoder.layers.3.adapters.4.sequential.0.bias', 'decoder.layers.1.adapters.4.sequential.2.weight', 'decoder.layers.5.adapters.2.sequential.2.weight', 'decoder.layers.2.adapters.3.sequential.0.weight', 'decoder.layers.5.adapters.0.sequential.0.bias', 'decoder.layers.0.adapters.0.sequential.2.weight', 'decoder.layers.2.adapters.4.sequential.0.weight', 'decoder.layers.5.adapters.3.sequential.2.weight', 'decoder.layers.1.adapters.3.sequential.2.bias', 'decoder.layers.3.adapters.3.sequential.0.bias', 'decoder.layers.5.adapters.4.sequential.2.weight', 'decoder.layers.5.adapters.0.sequential.0.weight', 'decoder.layers.3.linear.bias', 'decoder.layers.0.adapters.4.sequential.0.bias', 'decoder.layers.3.adapters.0.sequential.2.weight', 'decoder.layers.2.adapters.1.sequential.0.bias', 'decoder.layers.0.adapters.2.sequential.0.bias', 'decoder.layers.1.adapters.1.sequential.2.bias', 'decoder.layers.5.linear.bias', 'decoder.layers.3.adapters.0.sequential.0.weight', 'decoder.layers.1.adapters.4.sequential.0.bias', 'decoder.layers.2.adapters.0.sequential.2.weight', 'decoder.layers.1.adapters.1.sequential.0.bias', 'decoder.layers.3.adapters.1.sequential.0.weight', 'decoder.layers.5.linear.weight', 'decoder.layers.0.adapters.1.sequential.2.weight', 'decoder.layers.0.adapters.4.sequential.0.weight', 'decoder.layers.4.adapters.1.sequential.0.bias', 'decoder.layers.1.adapters.2.sequential.0.weight', 'decoder.layers.5.adapters.3.sequential.0.bias', 'decoder.layers.2.linear.weight', 'decoder.layers.4.adapters.0.sequential.2.weight', 'decoder.layers.1.ln_3.bias', 'decoder.layers.4.adapters.3.sequential.0.weight', 'decoder.layers.4.adapters.2.sequential.0.bias', 'decoder.layers.3.adapters.0.sequential.0.bias', 'decoder.layers.0.adapters.1.sequential.0.bias', 'decoder.layers.0.ln_3.weight', 'decoder.layers.5.adapters.0.sequential.2.bias', 'decoder.layers.0.adapters.3.sequential.2.weight', 'decoder.layers.2.adapters.1.sequential.2.weight', 'decoder.layers.3.adapters.1.sequential.2.weight', 'decoder.layers.4.adapters.0.sequential.2.bias', 'decoder.layers.3.ln_3.weight', 'decoder.layers.4.adapters.2.sequential.0.weight', 'decoder.layers.1.adapters.2.sequential.0.bias', 'decoder.layers.1.linear.bias', 'decoder.layers.2.ln_3.weight', 'decoder.layers.5.adapters.2.sequential.0.weight', 'decoder.layers.0.adapters.2.sequential.2.bias', 'decoder.layers.3.adapters.2.sequential.2.weight', 'decoder.layers.2.ln_3.bias', 'decoder.layers.5.adapters.2.sequential.2.bias', 'decoder.layers.3.ln_3.bias', 'decoder.layers.4.ln_3.bias', 'decoder.layers.5.adapters.2.sequential.0.bias', 'decoder.layers.2.adapters.2.sequential.2.bias', 'decoder.layers.4.linear.weight', 'decoder.layers.2.adapters.3.sequential.0.bias', 'decoder.layers.3.adapters.0.sequential.2.bias', 'decoder.layers.1.adapters.1.sequential.2.weight', 'decoder.layers.0.adapters.4.sequential.2.weight', 'decoder.layers.0.adapters.0.sequential.0.weight', 'decoder.layers.3.adapters.2.sequential.0.bias', 'decoder.layers.5.adapters.0.sequential.2.weight', 'decoder.layers.5.ln_3.bias', 'decoder.layers.4.adapters.4.sequential.0.bias', 'decoder.layers.4.adapters.4.sequential.0.weight', 'decoder.layers.0.adapters.0.sequential.2.bias', 'decoder.layers.4.adapters.3.sequential.2.bias', 'decoder.layers.1.adapters.4.sequential.2.bias', 'decoder.layers.1.adapters.0.sequential.0.bias', 'decoder.layers.0.adapters.4.sequential.2.bias', 'decoder.layers.0.adapters.3.sequential.2.bias', 'decoder.layers.1.adapters.0.sequential.2.weight', 'decoder.layers.1.adapters.0.sequential.0.weight', 'decoder.layers.1.linear.weight', 'decoder.layers.5.adapters.1.sequential.0.weight', 'decoder.layers.1.adapters.3.sequential.2.weight', 'decoder.layers.4.adapters.1.sequential.2.bias', 'decoder.layers.3.adapters.4.sequential.2.bias', 'decoder.layers.5.adapters.1.sequential.2.bias', 'decoder.layers.4.adapters.0.sequential.0.bias', 'decoder.layers.2.adapters.2.sequential.0.bias', 'decoder.layers.2.adapters.0.sequential.2.bias', 'decoder.layers.3.adapters.2.sequential.0.weight', 'decoder.layers.2.adapters.1.sequential.2.bias', 'decoder.layers.4.adapters.2.sequential.2.weight', 'decoder.layers.5.ln_3.weight', 'decoder.layers.2.adapters.1.sequential.0.weight', 'decoder.layers.3.adapters.3.sequential.2.weight', 'decoder.layers.3.adapters.3.sequential.2.bias', 'decoder.layers.3.linear.weight', 'decoder.layers.1.adapters.2.sequential.2.bias', 'decoder.layers.5.adapters.4.sequential.0.bias', 'decoder.layers.2.adapters.0.sequential.0.weight', 'decoder.layers.1.ln_3.weight', 'decoder.layers.4.adapters.0.sequential.0.weight', 'decoder.layers.3.adapters.1.sequential.0.bias', 'decoder.layers.0.adapters.3.sequential.0.bias', 'decoder.layers.4.adapters.3.sequential.2.weight', 'decoder.layers.3.adapters.4.sequential.0.weight', 'decoder.layers.0.linear.weight', 'decoder.layers.5.adapters.3.sequential.0.weight', 'decoder.layers.1.adapters.0.sequential.2.bias', 'decoder.layers.0.adapters.1.sequential.2.bias', 'decoder.layers.2.adapters.3.sequential.2.weight', 'decoder.layers.3.adapters.1.sequential.2.bias', 'decoder.layers.2.adapters.4.sequential.2.weight', 'decoder.layers.2.adapters.0.sequential.0.bias', 'decoder.layers.1.adapters.2.sequential.2.weight', 'decoder.layers.2.adapters.2.sequential.2.weight', 'decoder.layers.0.adapters.0.sequential.0.bias', 'decoder.layers.2.adapters.4.sequential.0.bias', 'decoder.layers.1.adapters.1.sequential.0.weight', 'decoder.layers.2.adapters.2.sequential.0.weight', 'decoder.layers.4.adapters.3.sequential.0.bias', 'decoder.layers.3.adapters.2.sequential.2.bias', 'decoder.layers.4.adapters.4.sequential.2.weight', 'decoder.layers.1.adapters.3.sequential.0.weight', 'decoder.layers.3.adapters.3.sequential.0.weight', 'decoder.layers.1.adapters.4.sequential.0.weight', 'decoder.layers.2.linear.bias', 'decoder.layers.5.adapters.4.sequential.2.bias', 'decoder.layers.0.adapters.3.sequential.0.weight', 'decoder.layers.0.adapters.1.sequential.0.weight', 'decoder.layers.4.adapters.2.sequential.2.bias', 'decoder.layers.0.adapters.2.sequential.0.weight', 'decoder.layers.5.adapters.1.sequential.2.weight', 'decoder.layers.0.adapters.2.sequential.2.weight', 'decoder.layers.5.adapters.1.sequential.0.bias', 'decoder.layers.0.linear.bias', 'decoder.layers.4.linear.bias', 'decoder.layers.3.adapters.4.sequential.2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model + tokenizer\n"
     ]
    }
   ],
   "source": [
    "model_name='facebook/bart-base'\n",
    "#model_name='gpt2'\n",
    "device='cuda'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "print(\"Model + tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "26af0fa2-6ed9-4330-995d-9611e3263aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze model except head and adapters parameters\n",
    "for param in model.model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "for layer_index in range(6):\n",
    "    layer = model.model.decoder.layers[layer_index]\n",
    "    for part in [layer.adapters, layer.ln_3, layer.linear, layer.softmax]:\n",
    "        for param in part.parameters():\n",
    "            param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b06c64c-f93a-4cab-8f53-fc2d7e112cfa",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efb1d1c1-7dec-4096-9c21-d0c192f17fb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a9ea61d2f4e7a15c\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-a9ea61d2f4e7a15c/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65cf71ea56dc40fb965f4008f445cb21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-bfe3ce7955f4ec3f\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-bfe3ce7955f4ec3f/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3168f6da9c8940609f510ee02e0aee5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6c62c6b01d7a74cd\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-6c62c6b01d7a74cd/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ef3fa983b54eeaba435755381ed35d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ac5c9909ebd64193\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-ac5c9909ebd64193/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d49bf1a3e8a4df79180faa6e1dfad65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_dir= 'modified_dataset/'\n",
    "train_dict={}\n",
    "relations = ['Physical','Event', 'Intent','Reaction']\n",
    "for relation in relations:\n",
    "    train_dict[relation] = load_dataset('json', data_files={'train': f'{dataset_dir}{relation} train.json'}, download_mode= DownloadMode.REUSE_DATASET_IF_EXISTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91dd18cf-24fc-46a3-a8cb-a484c9125072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-a9ea61d2f4e7a15c/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-fc4caa6007dbf9e5.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-bfe3ce7955f4ec3f/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-24e56ffa2c841521.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-6c62c6b01d7a74cd/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-cf01bf2f244f700f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-ac5c9909ebd64193/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-3b1388a7e63f7c6a.arrow\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 64\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples['head'], text_target=examples['tail'], max_length=max_seq_length,\n",
    "                             truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "train_tok_dict={}\n",
    "for relation in relations:\n",
    "    train_tok_dict[relation] = train_dict[relation].map(\n",
    "        preprocess_function,\n",
    "        batched=True,  # num_proc=num_proc,\n",
    "        remove_columns=['head', 'tail'],\n",
    "        load_from_cache_file=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478dd527-0623-449a-8f89-6b92ec215b64",
   "metadata": {},
   "source": [
    "## Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee6c3673-9cc8-4466-8208-2b3ed526e0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-58f9268bc00d6442\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-58f9268bc00d6442/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04127e3bc7e4267b652b12ed7b99214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-bb7b8c53aeb03b18\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-bb7b8c53aeb03b18/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f803d258206d47a4befea5344f6f92e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-499d547d83530161\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-499d547d83530161/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f83ef376e434f40879b0048e2df1bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-79c5d3620c090446\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-79c5d3620c090446/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8fbd79075274b6aae85c0e66c6e8efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-58f9268bc00d6442/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-b8466aeb5d9a977b.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-bb7b8c53aeb03b18/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-6caa6a9eabea699e.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-499d547d83530161/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-06827029f485b6b6.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-79c5d3620c090446/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-6716832c84c52af8.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset_dir= 'modified_dataset/'\n",
    "test_dict={}\n",
    "relations = ['Physical','Event', 'Intent','Reaction']\n",
    "for relation in relations:\n",
    "    test_dict[relation] = load_dataset('json', data_files={'test': f'{dataset_dir}{relation} test.json'}, download_mode= DownloadMode.REUSE_DATASET_IF_EXISTS)\n",
    "\n",
    "max_seq_length = 64\n",
    "def preprocess_function(examples):\n",
    "    examples['tail']=['\\t'.join(x) for x in examples['tail']]\n",
    "    return examples\n",
    "\n",
    "for relation in relations:\n",
    "    test_dict[relation] = test_dict[relation].map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        load_from_cache_file=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2bb024-a32c-406a-98c1-b540ce912a16",
   "metadata": {},
   "source": [
    "## generation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b81214da-f2fe-4d8f-92df-95236f066e1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical\n",
      "['a teabag can be used to']\n",
      "['useful']\n",
      "labels\n",
      "[['hold the loose leaves', 'increase the strength of the drink', 'put into mug for steeping', 'ease eye pain', 'make plants grow better', 'wipe stains off of steel', 'fertilize plants', 'deororize garbage', 'feed slugs', 'get rid of bags under eyes', 'make a nice aroma in the home', 'get herbal nutrients', 'make sweet tea with', 'sniff']]\n",
      "Event\n",
      "[\"PersonX asks PersonY's dad. Before that,\"]\n",
      "['curious']\n",
      "labels\n",
      "[['PersonX wants to ask an important question']]\n",
      "Intent\n",
      "['PersonX tells PersonY a little. After, others will want to']\n",
      "['cries']\n",
      "labels\n",
      "[[' know more about the matter', ' entice him into letting out the whole secret', ' ask questions', 'PersonX to tell everything', ' know the whole story', ' help PersonX']]\n",
      "Reaction\n",
      "[\"PersonX waters PersonX's garden. PersonX will be\"]\n",
      "['satisfied']\n",
      "labels\n",
      "[['dutiful', 'satisfied', 'proud', 'expectant', 'hopeful', 'nurturing']]\n"
     ]
    }
   ],
   "source": [
    "for relation in relations:\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dict[relation]['test'],\n",
    "        batch_size=1,\n",
    "    )\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = tokenizer(batch['head'], padding=True, return_tensors=\"pt\").to(device)\n",
    "        generations = model.generate(**input_ids)\n",
    "        print(f\"{relation}\")\n",
    "        print(batch['head'])\n",
    "        print(tokenizer.batch_decode(generations, skip_special_tokens=True))\n",
    "        print('labels')\n",
    "        print([s.split('\\t') for s in batch['tail']])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd0b01d-13f8-43fe-a60a-93b8fb88a670",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Incremental Training + Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf2c53c-7d59-48c0-ac5e-4867c0c312d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/ml/MasterProject/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a54095b55024c469460ca0fa0a46c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8379 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "metric=evaluate.load('bleu')\n",
    "USE_REC_ADAM =False\n",
    "output_dir= 'rec_adam/' if USE_REC_ADAM else 'UMA/' \n",
    "for train_relation in relations:\n",
    "    os.makedirs(f'{output_dir}{train_relation}', exist_ok=True)\n",
    "    \n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_tok_dict[train_relation]['train'],\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator,\n",
    "        batch_size=64,\n",
    "    )\n",
    "    if USE_REC_ADAM and train_relation != relations[0]:\n",
    "        optimizer = RecAdam(model.parameters(), lr=1e-3, pretrain_params= list(model.parameters()))\n",
    "    else:\n",
    "        optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "\n",
    "    num_train_epochs = 3\n",
    "    num_update_steps_per_epoch = len(train_dataloader)\n",
    "    num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_train_epochs):\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    \n",
    "    model.eval() # put in testing mode (dropout modules are deactivated)\n",
    "    for test_relation in relations:\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dict[test_relation]['test'],\n",
    "            batch_size=64,\n",
    "        )\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = tokenizer(batch['head'], padding=True, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                generations = model.generate(**input_ids)\n",
    "            decoded_gens= tokenizer.batch_decode(generations, skip_special_tokens=True)\n",
    "            labels = [s.split('\\t') for s in batch['tail']]\n",
    "            metric.add_batch(predictions=decoded_gens, references=labels)\n",
    "        results = metric.compute(max_order=2)\n",
    "        results['blue-1']=results['brevity_penalty']*results['precisions'][0]\n",
    "        f = open(f'{output_dir}results.txt', \"a\")\n",
    "        f.write(f'{train_relation} test on {test_relation} \\n {results} \\n')\n",
    "        f.close()   \n",
    "    \n",
    "        \n",
    "\n",
    "    model.save_pretrained(f'{output_dir}{train_relation}')    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cfb343-9fed-4ba3-a951-9516c2b5d470",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Elastic Weight Consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf34fd6d-d7dd-4fc5-a998-a4c10a2cca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "optpar_dict = {}\n",
    "fisher_dict = {}\n",
    "def on_task_update(train_dataloader):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # accumulating gradients\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "    optpar_dict.clear()\n",
    "    fisher_dict.clear()\n",
    "    # gradients accumulated is used to compute fisher\n",
    "    for name, param in model.named_parameters():\n",
    "        optpar_dict[name] = param.data.clone()\n",
    "        fisher_dict[name] = param.grad.data.clone().pow(2)\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a65156-76f3-4306-a664-e11d7827cb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metric=evaluate.load('bleu')\n",
    "ewc_lambda=1000\n",
    "output_dir= f'ewc_l={ewc_lambda}/'\n",
    "for train_relation in relations:\n",
    "    os.makedirs(f'{output_dir}{train_relation}', exist_ok=False)\n",
    "    \n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_tok_dict[train_relation]['train'],\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator,\n",
    "        batch_size=64,\n",
    "    )\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "\n",
    "    num_train_epochs = 3\n",
    "    num_update_steps_per_epoch = len(train_dataloader)\n",
    "    num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_train_epochs):\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            if train_relation!= relations[0]:\n",
    "                #EWC penalty\n",
    "                for name, param in model.named_parameters():\n",
    "                    fisher = fisher_dict[name]\n",
    "                    optpar = optpar_dict[name]\n",
    "                    loss += (fisher * (optpar - param).pow(2)).sum() * ewc_lambda\n",
    "\n",
    "            loss.backward()            \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "    \n",
    "    on_task_update(train_dataloader)\n",
    "    \n",
    "    model.eval() # put in testing mode (dropout modules are deactivated)\n",
    "    for test_relation in relations:\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dict[test_relation]['test'],\n",
    "            batch_size=64,\n",
    "        )\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = tokenizer(batch['head'], padding=True, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                generations = model.generate(**input_ids)\n",
    "            decoded_gens= tokenizer.batch_decode(generations, skip_special_tokens=True)\n",
    "            labels = [s.split('\\t') for s in batch['tail']]\n",
    "            metric.add_batch(predictions=decoded_gens, references=labels)\n",
    "        results = metric.compute(max_order=2)\n",
    "        results['blue-1']=results['brevity_penalty']*results['precisions'][0]\n",
    "        f = open(f'{output_dir}results.txt', \"a\")\n",
    "        f.write(f'{train_relation} test on {test_relation} \\n {results} \\n')\n",
    "        f.close()   \n",
    "    \n",
    "        \n",
    "\n",
    "    model.save_pretrained(f'{output_dir}{train_relation}')    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0394e305-8234-4e39-bf37-549c728772db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
