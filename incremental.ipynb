{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0fede98-41c8-4073-9d06-b840ac2ece0a",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c27ca958-9576-45b0-9998-ea983e622807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import cuda\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import evaluate\n",
    "from transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainingArguments, pipeline, AdamW, get_scheduler\n",
    "from argparse import ArgumentParser\n",
    "import numpy as np\n",
    "from datasets import load_dataset, DownloadMode\n",
    "from tqdm.auto import tqdm\n",
    "from script.rec_adam import RecAdam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035d4171-a787-4048-be3a-4b224cc5114f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## model + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12a9cb63-f852-4990-b0c1-52d865b62193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model + tokenizer\n"
     ]
    }
   ],
   "source": [
    "model_name='facebook/bart-base'\n",
    "device='cuda'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "print(\"Model + tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b06c64c-f93a-4cab-8f53-fc2d7e112cfa",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efb1d1c1-7dec-4096-9c21-d0c192f17fb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a65f559f25e8bb21\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-a65f559f25e8bb21/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a3c125920549378aa75e52db6aa879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ebda442e4b7407f2\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-ebda442e4b7407f2/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95cbf38dbb4d40699a0e40077e7dbe7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-65647a2d2c9d2c86\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-65647a2d2c9d2c86/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068bd22136b84559bee709d74fdb0a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c12fb152fedc44d5\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-c12fb152fedc44d5/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7063a42c32e41309163b535d878ba18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-a65f559f25e8bb21/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-146e8d6bd159c98c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-ebda442e4b7407f2/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-3c93bc7850aa0341.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-65647a2d2c9d2c86/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f79984373479e4aa.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-c12fb152fedc44d5/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-18049fc3b76763fd.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset_dir= 'modified_dataset/'\n",
    "train_dict={}\n",
    "relations = ['Physical','Event', 'Intent','Reaction']\n",
    "for relation in relations:\n",
    "    train_dict[relation] = load_dataset('json', data_files={'train': f'{dataset_dir}{relation} train.json'}, download_mode= DownloadMode.REUSE_DATASET_IF_EXISTS)\n",
    "\n",
    "max_seq_length = 64\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples['head'], text_target=examples['tail'], max_length=max_seq_length,\n",
    "                             truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "train_tok_dict={}\n",
    "for relation in relations:\n",
    "    train_tok_dict[relation] = train_dict[relation].map(\n",
    "        preprocess_function,\n",
    "        batched=True,  # num_proc=num_proc,\n",
    "        remove_columns=['head', 'tail'],\n",
    "        load_from_cache_file=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478dd527-0623-449a-8f89-6b92ec215b64",
   "metadata": {},
   "source": [
    "## Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee6c3673-9cc8-4466-8208-2b3ed526e0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-1036bc633c4cf542\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1036bc633c4cf542/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd94c0fb949c4f29b021192b8985b34c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4a7ea5f69cab20da\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-4a7ea5f69cab20da/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb966fda96aa4b50a2edcfcb892a5adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a44d390d889f4596\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-a44d390d889f4596/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ce6c57e2d74fa9b3d22650172c0c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-67e1f0b46e31265d\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-67e1f0b46e31265d/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccfd884a23e2477b82df94b15125d363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1036bc633c4cf542/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-43cd2fa2060b6560.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-4a7ea5f69cab20da/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-c6e3e0187d4f2d58.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-a44d390d889f4596/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-b7a860e5449eb85a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-67e1f0b46e31265d/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-9ea25f7820045350.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset_dir= 'modified_dataset/'\n",
    "test_dict={}\n",
    "relations = ['Physical','Event', 'Intent','Reaction']\n",
    "for relation in relations:\n",
    "    test_dict[relation] = load_dataset('json', data_files={'test': f'{dataset_dir}{relation} test.json'}, download_mode= DownloadMode.REUSE_DATASET_IF_EXISTS)\n",
    "\n",
    "max_seq_length = 64\n",
    "def preprocess_function(examples):\n",
    "    examples['tail']=['\\t'.join(x) for x in examples['tail']]\n",
    "    return examples\n",
    "\n",
    "for relation in relations:\n",
    "    test_dict[relation] = test_dict[relation].map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        load_from_cache_file=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2bb024-a32c-406a-98c1-b540ce912a16",
   "metadata": {},
   "source": [
    "## generation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b81214da-f2fe-4d8f-92df-95236f066e1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.9/site-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical\n",
      "['You are likely to find a construction in a']\n",
      "['You are likely to find a construction in a']\n",
      "labels\n",
      "[['roadblock']]\n",
      "Event\n",
      "['PersonX wants to hurt PersonY. Before that,']\n",
      "['PersonX wants to hurt PersonY. Before that,']\n",
      "labels\n",
      "[['PersonX gets punched by PersonY']]\n",
      "Intent\n",
      "[\"PersonX preaches god 's ___. PersonX did this to\"]\n",
      "[\"PersonX preaches god's ___. PersonX did this to\"]\n",
      "labels\n",
      "[['peace']]\n",
      "Reaction\n",
      "[\"PersonX sees PersonY in PersonX's office. PersonX will be\"]\n",
      "[\"PersonX sees PersonY in PersonX's office. PersonX will be\"]\n",
      "labels\n",
      "[['surprised', 'courteous', 'interested']]\n"
     ]
    }
   ],
   "source": [
    "for relation in relations:\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dict[relation]['test'],\n",
    "        batch_size=1,\n",
    "    )\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = tokenizer(batch['head'], padding=True, return_tensors=\"pt\").to(device)\n",
    "        generations = model.generate(**input_ids)\n",
    "        print(f\"{relation}\")\n",
    "        print(batch['head'])\n",
    "        print(tokenizer.batch_decode(generations, skip_special_tokens=True))\n",
    "        print('labels')\n",
    "        print([s.split('\\t') for s in batch['tail']])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd0b01d-13f8-43fe-a60a-93b8fb88a670",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Incremental Training + Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf2c53c-7d59-48c0-ac5e-4867c0c312d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric=evaluate.load('bleu')\n",
    "USE_REC_ADAM =True\n",
    "output_dir= 'rec_adam/' if USE_REC_ADAM else 'incremental/' \n",
    "for train_relation in relations:\n",
    "    os.makedirs(f'{output_dir}{train_relation}', exist_ok=True)\n",
    "    \n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_tok_dict[train_relation]['train'],\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator,\n",
    "        batch_size=64,\n",
    "    )\n",
    "    if USE_REC_ADAM and train_relation != relations[0]:\n",
    "        optimizer = RecAdam(model.parameters(), lr=1e-3, pretrain_params= list(model.parameters()))\n",
    "    else:\n",
    "        optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "\n",
    "    num_train_epochs = 3\n",
    "    num_update_steps_per_epoch = len(train_dataloader)\n",
    "    num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_train_epochs):\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    \n",
    "    model.eval() # put in testing mode (dropout modules are deactivated)\n",
    "    for test_relation in relations:\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dict[test_relation]['test'],\n",
    "            batch_size=64,\n",
    "        )\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = tokenizer(batch['head'], padding=True, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                generations = model.generate(**input_ids)\n",
    "            decoded_gens= tokenizer.batch_decode(generations, skip_special_tokens=True)\n",
    "            labels = [s.split('\\t') for s in batch['tail']]\n",
    "            metric.add_batch(predictions=decoded_gens, references=labels)\n",
    "        results = metric.compute(max_order=2)\n",
    "        results['blue-1']=results['brevity_penalty']*results['precisions'][0]\n",
    "        f = open(f'{output_dir}results.txt', \"a\")\n",
    "        f.write(f'{train_relation} test on {test_relation} \\n {results} \\n')\n",
    "        f.close()   \n",
    "    \n",
    "        \n",
    "\n",
    "    model.save_pretrained(f'{output_dir}{train_relation}')    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cfb343-9fed-4ba3-a951-9516c2b5d470",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Elastic Weight Consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf34fd6d-d7dd-4fc5-a998-a4c10a2cca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "optpar_dict = {}\n",
    "fisher_dict = {}\n",
    "def on_task_update(train_dataloader):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # accumulating gradients\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "    optpar_dict.clear()\n",
    "    fisher_dict.clear()\n",
    "    # gradients accumulated is used to compute fisher\n",
    "    for name, param in model.named_parameters():\n",
    "        optpar_dict[name] = param.data.clone()\n",
    "        fisher_dict[name] = param.grad.data.clone().pow(2)\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a65156-76f3-4306-a664-e11d7827cb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9330b19b14e445f7a192f5e00592a6ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8379 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metric=evaluate.load('bleu')\n",
    "ewc_lambda=1000\n",
    "output_dir= f'ewc_l={ewc_lambda}/'\n",
    "for train_relation in relations:\n",
    "    os.makedirs(f'{output_dir}{train_relation}', exist_ok=False)\n",
    "    \n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_tok_dict[train_relation]['train'],\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator,\n",
    "        batch_size=64,\n",
    "    )\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "\n",
    "    num_train_epochs = 3\n",
    "    num_update_steps_per_epoch = len(train_dataloader)\n",
    "    num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_train_epochs):\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            if train_relation!= relations[0]:\n",
    "                #EWC penalty\n",
    "                for name, param in model.named_parameters():\n",
    "                    fisher = fisher_dict[name]\n",
    "                    optpar = optpar_dict[name]\n",
    "                    loss += (fisher * (optpar - param).pow(2)).sum() * ewc_lambda\n",
    "\n",
    "            loss.backward()            \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "    \n",
    "    on_task_update(train_dataloader)\n",
    "    \n",
    "    model.eval() # put in testing mode (dropout modules are deactivated)\n",
    "    for test_relation in relations:\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dict[test_relation]['test'],\n",
    "            batch_size=64,\n",
    "        )\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = tokenizer(batch['head'], padding=True, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                generations = model.generate(**input_ids)\n",
    "            decoded_gens= tokenizer.batch_decode(generations, skip_special_tokens=True)\n",
    "            labels = [s.split('\\t') for s in batch['tail']]\n",
    "            metric.add_batch(predictions=decoded_gens, references=labels)\n",
    "        results = metric.compute(max_order=2)\n",
    "        results['blue-1']=results['brevity_penalty']*results['precisions'][0]\n",
    "        f = open(f'{output_dir}results.txt', \"a\")\n",
    "        f.write(f'{train_relation} test on {test_relation} \\n {results} \\n')\n",
    "        f.close()   \n",
    "    \n",
    "        \n",
    "\n",
    "    model.save_pretrained(f'{output_dir}{train_relation}')    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0394e305-8234-4e39-bf37-549c728772db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
